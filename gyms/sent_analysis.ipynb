{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28053f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import json\n",
    "import pdb\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from senticnet.babelsenticnet import BabelSenticNet\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458d97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/home/accg14/Documents/FIng/Taller ARS/Entrega/AdD-2021-Instagram-research/gyms/sources/crossfitdelsur/crossfitdelsur.json'\n",
    "UNWANTED_CHARS = ['!', ',', '\"', '-', '...','–','XD', 'xD', '¿', '?', '—', '\\n', \"#\", '¡', ':', \"“\", '.', '(', ')',\"¬¬\", \"\\('.')/\", \"*\", '\\n', '»', '\\x97', '\\x85']\n",
    "SPANISH_STOP_WORDS = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d0c8a2",
   "metadata": {},
   "source": [
    "# Pre processing stage\n",
    "En esta etapa, se recuperan los comentarios de las publicaciones de las instituciones (gimnasios) y son procesados, lo cual incluye:\n",
    "1. Recuperar comentarios (utilizar yield permite mejora de performance) y agruparlos en una estructura.\n",
    "2. Remover caracteres que generan inputs de baja calidad, emojis, no ascii, signos de exclamación, de interrogación entre otros (todos aquellos no alfabeticos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cbef34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts_comments(file):\n",
    "    with open(file) as json_file:\n",
    "        institute_posts = json.load(json_file)\n",
    "        for posts in institute_posts['GraphImages']:\n",
    "            comments_data = posts['comments']\n",
    "            for comment_data in comments_data['data']:\n",
    "                yield(comment_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04264a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "for comment in get_posts_comments(file):\n",
    "    comments.append(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfcaf15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_chars(token):\n",
    "    for char in UNWANTED_CHARS:\n",
    "        token = token.replace(char, ' ')\n",
    "        token = re.sub('@\\w*', '', token)\n",
    "        token = re.sub('\\$', ' ', token)\n",
    "        token = re.sub('\\d', '', token)\n",
    "        \n",
    "        return token\n",
    "\n",
    "def remove_emoji(token):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U000024C2-\\U0001F251\"\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', token)\n",
    "\n",
    "def remove_non_ascii(token):\n",
    "    try:\n",
    "        _ = token.encode('ascii')\n",
    "        return token\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def remove_spanish_stop_words(token):\n",
    "    if (token in stopwords.words('spanish')):\n",
    "        return None\n",
    "    else:\n",
    "        return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb837c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_comment(comment):\n",
    "    comment = comment.split(\" \")\n",
    "\n",
    "    comment = list(map(remove_emoji, comment))\n",
    "    comment = list(map(remove_unwanted_chars, comment))\n",
    "\n",
    "    comment = list(filter(lambda x: len(x)>0, comment))\n",
    "    comment = list(map(remove_spanish_stop_words, comment))\n",
    "    comment = list(filter(lambda x: x is not None, comment))\n",
    "    comment = list(map(remove_non_ascii, comment))\n",
    "    comment = list(filter(lambda x: x is not None, comment))\n",
    "    comment = list(map(lambda x: x.lower(), comment))\n",
    "    comment = list(map(remove_unwanted_chars, comment))\n",
    "    comment = list(map(lambda x: x.replace(\" \", \"\"), comment))\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff879a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deseando', 'volver']\n",
      "['todos', 'queremos', 'volver']\n",
      "['si', 'por', 'favor']\n",
      "['minutos', 'seguro']\n",
      "['mucho']\n",
      "['clases', 'gente']\n",
      "['se', 'gente,', 'volveremos']\n",
      "['estaremos', 'pronto']\n",
      "['es', 'dar', 'sigo', 'pasos']\n",
      "['despegado']\n",
      "['vamo', 'vamoooo', 'siempre', 'apoyando', 'movimiento']\n",
      "['crack']\n",
      "['genios', 'vamos', 'am']\n",
      "['grande', 'javi', '']\n",
      "['am', 'vamos']\n",
      "['vamo', 'arriba']\n",
      "['vamo', 'arriba', 'javi', 'estaremos', 'firmes', 'rulo', 'estatua']\n",
      "['...', 'apoyo', 'comunidad,', 'abrazo', 'grande', 'seguir', 'empujando', \"pa'delante\", 'crossfitdelsur']\n",
      "['grandes', 'genial', 'iniciativa']\n",
      "['chicos', 'pueden', 'poner', 'link', 'canal', 'youtube?', 'me', 'costando', 'encontrarlo']\n",
      "['que', 'tremendo']\n",
      "['excelente,', 'paso', 'eso.', 'mismo,', 'mas', 'q', 'agradecida']\n",
      "['por', 'suerte', 'procesos', 'caidas', 'uds', 'sostenernos', 'recordarnos', 'si', 'puede']\n",
      "['es', 'parte', 'camino,', 'importa', 'cuantas', 'veces', 'quedes', 'rodillas,', 'importa', 'veces', 'levantas', 'rock']\n",
      "['feliz', 'chicas', 'crofiters']\n",
      "['gracias', 'fuertes', 'bellas']\n",
      "['vero', 'rompe', '']\n",
      "['dos', 'grandes', 'crossfit', 'sur']\n",
      "['el', 'mejor', 'lugar', 'mundo', 'mejores', 'profesionales', 'infinitas', 'gracias', 'siempre']\n",
      "['me', 'pasar', 'costos', 'horarios', 'privado?', 'gracias']\n",
      "['saludos,', 'equipo', 'hemos', 'oportunidad', 'ver', 'perfil', 'encanta', 'concepto', 'entrenamiento', 'trabajo', 'gimnasiolos', 'felicitamos', 'les', 'dejado', 'mensaje', 'bandeja', 'entrada', 'propuesta', 'interesante.', 'estamos', 'seguros', 'los', 'invitamos', 'leerla']\n",
      "['dm', 'us']\n",
      "['love', 'it', 'dm']\n",
      "['love', 'it', ',', 'dm']\n",
      "['la', 'paz']\n",
      "['vamo', 'arriba', 'feliz']\n",
      "['feliz']\n",
      "['feliz', 'navidad', 'todos']\n",
      "['muy', 'feliz', 'navidad']\n",
      "['lo', 'mejor', 'mejor', 'navidad', 'crofitt', 'sur', 'abrazo']\n",
      "['gracias', 'feliz', 'navidad', 'todos']\n",
      "['feliz', 'navidad']\n",
      "['hay', 'problema', 'app', 'aparece']\n",
      "['despegada', 'atleta']\n",
      "['no', 'importa', 'asegurado,', 'gran', 'equipo,', 've', 'reflejado', 'logrado', 'especial', 'brindan.', 'a', 'todos', 'gracias,', 'junto', 'ustedes', 'con', 'pasa', 'excelente', 'sea', 'tremenda']\n",
      "['siempre']\n",
      "['con', 'disfrutando', 'entrenar', 'aire', 'libre', 'nuevo,', 'misma', 'gente', 'linda']\n",
      "['genia', 'siempre', 'mejor']\n",
      "['vaamooos', 'bandaa']\n",
      "['esta', 'genial', 'entrenar', 'lindo', 'wod']\n"
     ]
    }
   ],
   "source": [
    "sanitized_comments = []\n",
    "for comment in comments:\n",
    "    sanitized_comment = sanitize_comment(comment)\n",
    "    if len(sanitized_comment)>0:\n",
    "        sanitized_comments.append(sanitized_comment)\n",
    "        print(sanitized_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023719be",
   "metadata": {},
   "source": [
    "# Processing stage (encoding)\n",
    "En esta etapa, se obtiene el sentimiento relacionado a cada palabra del comentario de la siguiente forma:\n",
    "1. Si la palabra aparece en el conjunto, se recuperan su etiqueta y valor de polaridad.\n",
    "2. Si la palabra no aparece en el conjunto, se obtiene (cuando existe) el lema de la misma y se retornan etiqueta y valor de polaridad del lema (si existe).\n",
    "3. Cuando ninguna de las anteriores condiciones se cumple, se suprime la palabra del comentario.\n",
    "\n",
    "Algunos comentarios: el estudio del español a nivel computacional no ha llegado al estadio del ingles, por lo que no todas las palabras tienen un lema asociado, y expresiones tipicas de ciertos lugares (en este contexto UY, quedan por fuera del alcance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "164b253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e9c9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_lemma(token):\n",
    "    token = nlp(token)\n",
    "    token_lemma = [t.lemma_ for t in token]\n",
    "    if len(token_lemma)>0:\n",
    "        return token_lemma[0]\n",
    "    else:\n",
    "        return [token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f67d10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_token_polarity(token, token_lemma):\n",
    "    bsn = BabelSenticNet('es')\n",
    "    try:\n",
    "        polarity_label = bsn.polarity_label(token)\n",
    "        polarity_value = bsn.polarity_value(token)\n",
    "        return polarity_label, polarity_value\n",
    "    except:\n",
    "        try:\n",
    "            polarity_label = bsn.polarity_label(token_lemma)\n",
    "            polarity_value = bsn.polarity_value(token_lemma)\n",
    "            return polarity_label, polarity_value\n",
    "        except:        \n",
    "            return 'neutral', 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "949ce829",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_polarity_label = []\n",
    "comments_polarity_value = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00b6f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_classification(token):\n",
    "    token_lemma = get_token_lemma(token)\n",
    "    token_polarity_label, token_polarity_value = classify_token_polarity(token, token_lemma)\n",
    "    return token_polarity_label, token_polarity_value\n",
    "\n",
    "def get_comment_classification(comment_polarity_label, comment_polarity_value, variance=False):\n",
    "    polarity_label = max(set(comment_polarity_label), key = comment_polarity_label.count)\n",
    "    polarity_value = statistics.mean(comment_polarity_value)\n",
    "    if variance:\n",
    "        polarity_variance = statistics.variance(comment_polarity_value)\n",
    "        return polarity_label, polarity_value, polarity_variance\n",
    "    else:\n",
    "        return polarity_label, polarity_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb05e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sanitized_comment in sanitized_comments:\n",
    "    comment_polarity_label = []\n",
    "    comment_polarity_value = []\n",
    "    for token in sanitized_comment:\n",
    "        token_polarity_label, token_polarity_value = get_token_classification(token)\n",
    "\n",
    "        comment_polarity_label.append(token_polarity_label)\n",
    "        comment_polarity_value.append(token_polarity_value)\n",
    "\n",
    "    label, value = get_comment_classification(comment_polarity_label, comment_polarity_value, False)\n",
    "    comments_polarity_label.append(label)\n",
    "    comments_polarity_value.append(value)\n",
    "\n",
    "final_label, final_value, final_variance = get_comment_classification(comments_polarity_label, comments_polarity_value, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88f72f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "0.09664844209288653\n",
      "0.05220099494668499\n"
     ]
    }
   ],
   "source": [
    "print(final_label)\n",
    "print(final_value)\n",
    "print(final_variance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
