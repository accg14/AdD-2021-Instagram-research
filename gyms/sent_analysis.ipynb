{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0cbffab",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8f5ea",
   "metadata": {},
   "source": [
    "Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28053f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import json\n",
    "import pdb\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from senticnet.babelsenticnet import BabelSenticNet\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73165da",
   "metadata": {},
   "source": [
    "Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458d97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNWANTED_CHARS = ['!', ',', '\"', '-', '...','–','XD', 'xD', '¿', '?', '—', '\\n', \"#\", '¡', ':', \"“\", '.', '(', ')',\"¬¬\", \"\\('.')/\", \"*\", '\\n', '»', '\\x97', '\\x85']\n",
    "SPANISH_STOP_WORDS = stopwords.words('spanish')\n",
    "\n",
    "file = '/home/accg14/Documents/FIng/Taller ARS/Entrega/AdD-2021-Instagram-research/gyms/sources/crossfitdelsur/crossfitdelsur.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d0c8a2",
   "metadata": {},
   "source": [
    "# Preprocesamiento\n",
    "En esta etapa, se recuperan los comentarios de las publicaciones de las instituciones (gimnasios) y son procesados, lo cual incluye:\n",
    "1. Recuperar comentarios (utilizar yield permite mejora de performance) y agruparlos en una estructura.\n",
    "2. Remover caracteres que generan inputs de baja calidad, emojis, no ascii, signos de exclamación, de interrogación entre otros (todos aquellos no alfabeticos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9705b9aa",
   "metadata": {},
   "source": [
    "## Extracción desde las fuentes 🗄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cbef34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts_comments(file):\n",
    "    with open(file) as json_file:\n",
    "        institute_posts = json.load(json_file)\n",
    "        for posts in institute_posts['GraphImages']:\n",
    "            comments_data = posts['comments']\n",
    "            for comment_data in comments_data['data']:\n",
    "                yield(comment_data['text'])\n",
    "\n",
    "def get_posts_likes(file):\n",
    "    with open(file) as json_file:\n",
    "        institute_posts = json.load(json_file)\n",
    "        for posts in institute_posts['GraphImages']:\n",
    "            likes_received = posts['edge_media_preview_like']['count']\n",
    "            yield(likes_received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf7fbd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_comments = []\n",
    "posts_likes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04264a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for post_comment in get_posts_comments(file):\n",
    "    posts_comments.append(post_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c63d30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for post_like in get_posts_likes(file):\n",
    "    posts_likes.append(post_like)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a24a1e4",
   "metadata": {},
   "source": [
    "## Curación de los datos 🧹️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcaf15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_chars(token):\n",
    "    for char in UNWANTED_CHARS:\n",
    "        token = token.replace(char, ' ')\n",
    "        token = re.sub('@\\w*', '', token)\n",
    "        token = re.sub('\\$', ' ', token)\n",
    "        token = re.sub('\\d', '', token)\n",
    "        \n",
    "        return token\n",
    "\n",
    "def remove_emoji(token):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U000024C2-\\U0001F251\"\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', token)\n",
    "\n",
    "def remove_non_ascii(token):\n",
    "    try:\n",
    "        _ = token.encode('ascii')\n",
    "        return token\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def remove_spanish_stop_words(token):\n",
    "    if (token in stopwords.words('spanish')):\n",
    "        return None\n",
    "    else:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb837c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_comment(comment):\n",
    "    comment = comment.split(\" \")\n",
    "\n",
    "    comment = list(map(remove_emoji, comment))\n",
    "    comment = list(map(remove_unwanted_chars, comment))\n",
    "\n",
    "    comment = list(filter(lambda x: len(x)>0, comment))\n",
    "    comment = list(map(remove_spanish_stop_words, comment))\n",
    "    comment = list(filter(lambda x: x is not None, comment))\n",
    "    comment = list(map(remove_non_ascii, comment))\n",
    "    comment = list(filter(lambda x: x is not None, comment))\n",
    "    comment = list(map(lambda x: x.lower(), comment))\n",
    "    comment = list(map(remove_unwanted_chars, comment))\n",
    "    comment = list(map(lambda x: x.replace(\" \", \"\"), comment))\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd44af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitized_comments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff879a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comment in comments:\n",
    "    sanitized_comment = sanitize_comment(comment)\n",
    "    if len(sanitized_comment)>0:\n",
    "        sanitized_comments.append(sanitized_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023719be",
   "metadata": {},
   "source": [
    "# Obtención de la polaridad 😠😄\n",
    "En esta etapa, se obtiene el sentimiento relacionado a cada palabra del comentario de la siguiente forma:\n",
    "1. Si la palabra aparece en el conjunto, se recuperan su etiqueta y valor de polaridad.\n",
    "2. Si la palabra no aparece en el conjunto, se obtiene (cuando existe) el lema de la misma y se retornan etiqueta y valor de polaridad del lema (si existe).\n",
    "3. Cuando ninguna de las anteriores condiciones se cumple, se suprime la palabra del comentario.\n",
    "\n",
    "Algunos comentarios: el estudio del español a nivel computacional no ha llegado al estadio del ingles, por lo que no todas las palabras tienen un lema asociado, y expresiones tipicas de ciertos lugares (en este contexto UY, quedan por fuera del alcance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_lemma(token):\n",
    "    token = nlp(token)\n",
    "    token_lemma = [t.lemma_ for t in token]\n",
    "    if len(token_lemma)>0:\n",
    "        return token_lemma[0]\n",
    "    else:\n",
    "        return [token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_token_polarity(token, token_lemma):\n",
    "    bsn = BabelSenticNet('es')\n",
    "    try:\n",
    "        polarity_label = bsn.polarity_label(token)\n",
    "        polarity_value = bsn.polarity_value(token)\n",
    "        return polarity_label, polarity_value\n",
    "    except:\n",
    "        try:\n",
    "            polarity_label = bsn.polarity_label(token_lemma)\n",
    "            polarity_value = bsn.polarity_value(token_lemma)\n",
    "            return polarity_label, polarity_value\n",
    "        except:        \n",
    "            return 'neutral', 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949ce829",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_polarity_label = []\n",
    "comments_polarity_value = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_classification(token):\n",
    "    token_lemma = get_token_lemma(token)\n",
    "    token_polarity_label, token_polarity_value = classify_token_polarity(token, token_lemma)\n",
    "    return token_polarity_label, token_polarity_value\n",
    "\n",
    "def get_comment_classification(comment_polarity_label, comment_polarity_value, variance=False):\n",
    "    polarity_label = max(set(comment_polarity_label), key = comment_polarity_label.count)\n",
    "    polarity_value = statistics.mean(comment_polarity_value)\n",
    "    if variance:\n",
    "        polarity_variance = statistics.variance(comment_polarity_value)\n",
    "        return polarity_label, polarity_value, polarity_variance\n",
    "    else:\n",
    "        return polarity_label, polarity_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb05e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sanitized_comment in sanitized_comments:\n",
    "    comment_polarity_label = []\n",
    "    comment_polarity_value = []\n",
    "    for token in sanitized_comment:\n",
    "        token_polarity_label, token_polarity_value = get_token_classification(token)\n",
    "\n",
    "        comment_polarity_label.append(token_polarity_label)\n",
    "        comment_polarity_value.append(token_polarity_value)\n",
    "\n",
    "    label, value = get_comment_classification(comment_polarity_label, comment_polarity_value, False)\n",
    "    comments_polarity_label.append(label)\n",
    "    comments_polarity_value.append(value)\n",
    "\n",
    "final_label, final_value, final_variance = get_comment_classification(comments_polarity_label, comments_polarity_value, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f72f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+\"Clasificación final de los comentarios: \"+final_label+\"\\033[0m\")\n",
    "\n",
    "print(final_value)\n",
    "print(final_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76020dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
