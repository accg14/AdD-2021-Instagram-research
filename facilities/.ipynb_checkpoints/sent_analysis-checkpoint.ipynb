{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28053f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pdb\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from senticnet.babelsenticnet import BabelSenticNet\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458d97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/home/accg14/Documents/FIng/Taller ARS/Entrega/AdD-2021-Instagram-research/facilities/crossfitdelsur/crossfitdelsur.json'\n",
    "\n",
    "UNWANTED_CHARS = ['!', ',', '\"', '-', '...','–','XD', 'xD', '¿', '?', '—', '\\n', \"#\", '¡', ':', \"“\", '.', '(', ')',\"¬¬\", \"\\('.')/\", \"*\", '\\n', '»', '\\x97', '\\x85']\n",
    "\n",
    "SPANISH_STOP_WORDS = stopwords.words('spanish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cbef34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts_comments(file):\n",
    "    with open(file) as json_file:\n",
    "        institute_posts = json.load(json_file)\n",
    "        for posts in institute_posts['GraphImages']:\n",
    "            comments_data = posts['comments']\n",
    "            for comment_data in comments_data['data']:\n",
    "                yield(comment_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04264a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "for comment in get_posts_comments(file):\n",
    "    comments.append(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfcaf15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "def remove_unwanted_chars(token):\n",
    "    for char in UNWANTED_CHARS:\n",
    "        token = token.replace(char, ' ')\n",
    "        token = re.sub('@\\w*', '', token)\n",
    "        token = re.sub('\\$', ' ', token)\n",
    "        token = re.sub('\\d', '', token)\n",
    "        \n",
    "        return token\n",
    "\n",
    "def remove_emoji(token):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U000024C2-\\U0001F251\"\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', token)\n",
    "\n",
    "def remove_non_ascii(token):\n",
    "    try:\n",
    "        _ = token.encode('ascii')\n",
    "        return token\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def remove_spanish_stop_words(token):\n",
    "    if (token in stopwords.words('spanish')):\n",
    "        return None\n",
    "    else:\n",
    "        return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb837c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_comment(comment):\n",
    "    comment = comment.split(\" \")\n",
    "\n",
    "    comment = list(map(remove_emoji, comment))\n",
    "    comment = list(map(remove_unwanted_chars, comment))\n",
    "\n",
    "    comment = list(filter(lambda x: len(x)>0, comment))\n",
    "    comment = list(map(remove_spanish_stop_words, comment))\n",
    "    comment = list(filter(lambda x: x is not None, comment))\n",
    "    comment = list(map(remove_non_ascii, comment))\n",
    "    comment = list(filter(lambda x: x is not None, comment))\n",
    "    comment = list(map(lambda x: x.lower(), comment))\n",
    "    comment = list(map(remove_unwanted_chars, comment))\n",
    "    comment = list(map(lambda x: x.replace(\" \", \"\"), comment))\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff879a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deseando', 'volver']\n",
      "['todos', 'queremos', 'volver']\n",
      "['si', 'por', 'favor']\n",
      "['minutos', 'seguro']\n",
      "['mucho']\n",
      "['clases', 'gente']\n",
      "['se', 'gente,', 'volveremos']\n",
      "['estaremos', 'pronto']\n",
      "['es', 'dar', 'sigo', 'pasos']\n",
      "['despegado']\n",
      "['vamo', 'vamoooo', 'siempre', 'apoyando', 'movimiento']\n",
      "['crack']\n",
      "['genios', 'vamos', 'am']\n",
      "['grande', 'javi', '']\n",
      "['am', 'vamos']\n",
      "['vamo', 'arriba']\n",
      "['vamo', 'arriba', 'javi', 'estaremos', 'firmes', 'rulo', 'estatua']\n",
      "['...', 'apoyo', 'comunidad,', 'abrazo', 'grande', 'seguir', 'empujando', \"pa'delante\", 'crossfitdelsur']\n",
      "['grandes', 'genial', 'iniciativa']\n",
      "['chicos', 'pueden', 'poner', 'link', 'canal', 'youtube?', 'me', 'costando', 'encontrarlo']\n",
      "['que', 'tremendo']\n",
      "['excelente,', 'paso', 'eso.', 'mismo,', 'mas', 'q', 'agradecida']\n",
      "['por', 'suerte', 'procesos', 'caidas', 'uds', 'sostenernos', 'recordarnos', 'si', 'puede']\n",
      "['es', 'parte', 'camino,', 'importa', 'cuantas', 'veces', 'quedes', 'rodillas,', 'importa', 'veces', 'levantas', 'rock']\n",
      "['feliz', 'chicas', 'crofiters']\n",
      "['gracias', 'fuertes', 'bellas']\n",
      "['vero', 'rompe', '']\n",
      "['dos', 'grandes', 'crossfit', 'sur']\n",
      "['el', 'mejor', 'lugar', 'mundo', 'mejores', 'profesionales', 'infinitas', 'gracias', 'siempre']\n",
      "['me', 'pasar', 'costos', 'horarios', 'privado?', 'gracias']\n",
      "['saludos,', 'equipo', 'hemos', 'oportunidad', 'ver', 'perfil', 'encanta', 'concepto', 'entrenamiento', 'trabajo', 'gimnasiolos', 'felicitamos', 'les', 'dejado', 'mensaje', 'bandeja', 'entrada', 'propuesta', 'interesante.', 'estamos', 'seguros', 'los', 'invitamos', 'leerla']\n",
      "['dm', 'us']\n",
      "['love', 'it', 'dm']\n",
      "['love', 'it', ',', 'dm']\n",
      "['la', 'paz']\n",
      "['vamo', 'arriba', 'feliz']\n",
      "['feliz']\n",
      "['feliz', 'navidad', 'todos']\n",
      "['muy', 'feliz', 'navidad']\n",
      "['lo', 'mejor', 'mejor', 'navidad', 'crofitt', 'sur', 'abrazo']\n",
      "['gracias', 'feliz', 'navidad', 'todos']\n",
      "['feliz', 'navidad']\n",
      "['hay', 'problema', 'app', 'aparece']\n",
      "['despegada', 'atleta']\n",
      "['no', 'importa', 'asegurado,', 'gran', 'equipo,', 've', 'reflejado', 'logrado', 'especial', 'brindan.', 'a', 'todos', 'gracias,', 'junto', 'ustedes', 'con', 'pasa', 'excelente', 'sea', 'tremenda']\n",
      "['siempre']\n",
      "['con', 'disfrutando', 'entrenar', 'aire', 'libre', 'nuevo,', 'misma', 'gente', 'linda']\n",
      "['genia', 'siempre', 'mejor']\n",
      "['vaamooos', 'bandaa']\n",
      "['esta', 'genial', 'entrenar', 'lindo', 'wod']\n"
     ]
    }
   ],
   "source": [
    "sanitized_comments = []\n",
    "for comment in comments:\n",
    "    sanitized_comment = sanitize_comment(comment)\n",
    "    if len(sanitized_comment)>0:\n",
    "        sanitized_comments.append(sanitized_comment)\n",
    "        print(sanitized_comment)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9e0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "164b253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e9c9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_lemma(token):\n",
    "    doc = nlp(token)\n",
    "    result = [word.lemma_ for word in doc]\n",
    "    return result[0]\n",
    "\n",
    "def classify_token_polarity(token):\n",
    "    bsn = BabelSenticNet('es')\n",
    "    try:\n",
    "        polarity_label = bsn.polarity_label(token)\n",
    "        polarity_value = bsn.polarity_value(token)\n",
    "        return polarity_label, polarity_value\n",
    "    except:\n",
    "        return 0, 'neutral'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb05e049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: desear\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: volver\n",
      "word_polarity_label: positive\n",
      "word_polarity_value:0.078\n",
      "token: todo\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: querer\n",
      "word_polarity_label: positive\n",
      "word_polarity_value:0.614\n",
      "token: volver\n",
      "word_polarity_label: positive\n",
      "word_polarity_value:0.078\n",
      "token: si\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: por\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: favor\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: minuto\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: seguro\n",
      "word_polarity_label: positive\n",
      "word_polarity_value:0.251\n",
      "token: mucho\n",
      "word_polarity_label: positive\n",
      "word_polarity_value:0.258\n",
      "token: clase\n",
      "word_polarity_label: positive\n",
      "word_polarity_value:0.078\n",
      "token: gente\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: él\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: gente\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: volver\n",
      "word_polarity_label: positive\n",
      "word_polarity_value:0.078\n",
      "token: estar\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: pronto\n",
      "word_polarity_label: positive\n",
      "word_polarity_value:0.862\n",
      "token: ser\n",
      "word_polarity_label: positive\n",
      "word_polarity_value:0.112\n",
      "token: dar\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: seguir\n",
      "word_polarity_label: positive\n",
      "word_polarity_value:0.117\n",
      "token: paso\n",
      "word_polarity_label: positive\n",
      "word_polarity_value:0.627\n",
      "token: despegado\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: vamo\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: vamoooo\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: siempre\n",
      "word_polarity_label: negative\n",
      "word_polarity_value:-0.835\n",
      "token: apoyar\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: movimiento\n",
      "word_polarity_label: negative\n",
      "word_polarity_value:-0.698\n",
      "token: crack\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: genio\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: ir\n",
      "word_polarity_label: positive\n",
      "word_polarity_value:0.596\n",
      "token: am\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: grande\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n",
      "token: javi\n",
      "word_polarity_label: 0\n",
      "word_polarity_value:neutral\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-470eafde5e7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msanitized_comment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msanitized_comments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msanitized_comment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_token_lemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtoken_polarity_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_polarity_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_token_polarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"token: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-57865a499b78>\u001b[0m in \u001b[0;36mget_token_lemma\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclassify_token_polarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for sanitized_comment in sanitized_comments:\n",
    "    for token in sanitized_comment:\n",
    "        token = get_token_lemma(token)\n",
    "        token_polarity_label, token_polarity_value = classify_token_polarity(token)\n",
    "        print(\"token: \"+ token)\n",
    "        print(\"word_polarity_label: \"+ str(token_polarity_label))\n",
    "        print(\"word_polarity_value:\"+ str(token_polarity_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f72f24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
